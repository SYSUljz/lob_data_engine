import pandas as pd
import numpy as np
import os
import joblib
import config
from src.hmm import MarketRegimeHMM
from src.hmm_eval import HMMEvaluator

def interpret_regimes(stats):
    """
    æ ¹æ®ç»Ÿè®¡æ•°æ®è‡ªåŠ¨ç»™çŠ¶æ€æ‰“æ ‡ç­¾ (Heuristic Logic)
    """
    labels = {}
    # æ‰¾åˆ°æ”¶ç›Šç‡æœ€é«˜å’Œæœ€ä½çš„çŠ¶æ€
    bull_state = stats['log_ret']['mean'].idxmax()
    bear_state = stats['log_ret']['mean'].idxmin()
    
    # åœ¨å‰©ä¸‹çš„çŠ¶æ€ä¸­å¯»æ‰¾ç‰¹å¾ï¼Œæˆ–è€…ç»†åˆ†
    for state in stats.index:
        mean_ret = stats.loc[state, ('log_ret', 'mean')]
        std_vol = stats.loc[state, ('vol_long', 'mean')]
        dur_dev = stats.loc[state, ('duration_deviation', 'mean')]
        
        # åŸºç¡€æ ‡ç­¾
        if state == bull_state:
            label = "Bull"
        elif state == bear_state:
            label = "Bear"
        else:
            label = "Sideways/Neutral"
            
        # é™„åŠ ç‰¹å¾æè¿°
        # å¦‚æœæ³¢åŠ¨ç‡æé«˜ï¼Œæ ‡è®°ä¸º High Vol
        if std_vol > stats[('vol_long', 'mean')].quantile(0.7):
            label = f"High-Vol {label}"
        # å¦‚æœ Duration Deviation å¾ˆè´Ÿï¼ˆå€¼å¾ˆå°ï¼‰ï¼Œè¯´æ˜ Bar äº§ç”Ÿæå¿«ï¼Œäº¤æ˜“æå…¶æ´»è·ƒ
        if dur_dev < 0:
            label = f"Active {label}"
            
        labels[state] = label
    return labels

def run_hmm_training():
    print("ğŸš€ Loading Dollar Bar training data...")
    train_path = os.path.join(config.PROCESSED_DATA_DIR, "train.parquet")
    
    if not os.path.exists(train_path):
        print(f"âŒ Error: {train_path} not found. Ensure ETL process is complete.")
        return

    df = pd.read_parquet(train_path)
    print(f"ğŸ“Š Loaded {len(df)} bars.")

    # 1. åˆå§‹åŒ–æ¨¡å‹
    # n_states=3 é€šå¸¸æ˜¯ Dollar Bar ä¸‹æœ€ç¨³å¥çš„é€‰æ‹©
    n_states = 3
    print(f"ğŸ¤– Initializing HMM with {n_states} states (diag covariance)...")
    hmm_model = MarketRegimeHMM(n_components=n_states, n_iter=100)

    # 2. è®­ç»ƒæ¨¡å‹ (å†…éƒ¨å·²åŒ…å« StandardScaler å’Œ State Remapping)
    print("âš™ï¸  Fitting model and optimizing state transitions...")
    hmm_model.train(df)

    # 3. é¢„æµ‹å¹¶å¹³æ»‘çŠ¶æ€
    # ä½¿ç”¨ smoothing=True è§£å†³ä½ æ‹…å¿ƒçš„â€œç±»åˆ«ä¸è¿ç»­â€é—®é¢˜
    print("ğŸ“ˆ Predicting and smoothing market regimes...")
    df_labeled = hmm_model.predict(df, smoothing=True, kernel_size=5)

    # 4. åˆ†æçŠ¶æ€å¹¶è‡ªåŠ¨è§£é‡Š
    print("\nğŸ§ State Statistical Analysis:")
    stats = hmm_model.get_state_stats(df_labeled)
    print(stats)
    
    regime_labels = interpret_regimes(stats)
    print("\nğŸ·ï¸  Identified Regimes:")
    for state, name in regime_labels.items():
        print(f"State {state}: {name}")

    # --- NEW: Evaluation Metrics ---
    print("\nğŸ•µï¸  Running Advanced Evaluation Metrics...")
    
    # helper to get feature names used by model
    _, feature_cols = hmm_model.prepare_features(df)
    
    evaluator = HMMEvaluator(hmm_model, df_labeled, feature_cols)
    eval_metrics = evaluator.evaluate_all()
    
    print("\n--- 1. Separation Metrics ---")
    print(f"Log-Likelihood per Sample: {eval_metrics['separation']['log_likelihood_per_sample']:.4f}")
    print(f"Avg Mahalanobis Distance:  {eval_metrics['separation']['mahalanobis_distance_mean']:.4f}")
    
    print("\n--- 2. Stability Metrics ---")
    print(f"Mean Regime Duration:      {eval_metrics['stability']['mean_regime_duration']:.2f} samples")
    print(f"Transition Matrix Entropy: {eval_metrics['stability']['transition_matrix_entropy']:.4f}")
    
    # User constraint check
    if eval_metrics['stability']['mean_regime_duration'] < 5:
        print("âš ï¸  WARNING: Mean Regime Duration is < 5 bars. Consider increasing penalty or smoothing.")
        
    print("\n--- 3. Predictive Power ---")
    print(f"Mutual Information (MI):   {eval_metrics['predictive']['mutual_information']:.4f}")
    print(f"Var Reduction (R2-like):   {eval_metrics['predictive']['variance_reduction']:.4%}")
    
    print("\n--- 4. Feature Importance ---")
    feat_imp = evaluator.feature_importance()
    
    # 1. Global Importance
    print("\n   [Global Permutation Importance]")
    sorted_imp = sorted(feat_imp['permutation_importance'].items(), key=lambda x: x[1], reverse=True)
    for feat, score in sorted_imp:
        print(f"   {feat:<25}: {score:.4%}")
        
    # 2. State Characteristics
    print("\n   [State Key Characteristics (Top 3 Deviation)]")
    features_list = feat_imp['state_z_scores'].keys()
    
    for s in features_list:
        z_scores = feat_imp['state_z_scores'][s]
        # Sort by absolute Z-score to find most "defining" features (positive or negative)
        sorted_z = sorted(z_scores.items(), key=lambda x: abs(x[1]), reverse=True)[:3]
        
        desc = []
        for feat, z in sorted_z:
            direction = "High" if z > 0 else "Low"
            desc.append(f"{direction} {feat} ({z:+.2f})")
            
        print(f"   State {s}: {', '.join(desc)}")
        
    # -------------------------------
    
    # 5. ä¿å­˜æ¨¡å‹åŠå…ƒæ•°æ®
    # å°†æ ‡ç­¾æ˜ å°„ä¸€èµ·ä¿å­˜ï¼Œæ–¹ä¾¿å›æµ‹è°ƒç”¨
    model_data = {
        'model': hmm_model,
        'regime_labels': regime_labels,
        'feature_stats': stats
    }
    
    model_path = os.path.join(config.PROCESSED_DATA_DIR, "hmm_model.pkl")
    print(f"\nğŸ’¾ Saving comprehensive model packet to {model_path}...")
    joblib.dump(model_data, model_path)
    
    print("âœ… Done! Regime classifier is ready for backtesting.")

if __name__ == "__main__":
    run_hmm_training()